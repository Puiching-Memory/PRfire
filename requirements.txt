# python==3.11
# torch==2.5.1(cu124)
transformers==4.37.2
huggingface_hub==0.27.1
lmdeploy==0.7.0
ultralytics==8.3.73
fastapi==0.115.8
uvicorn[standard]==0.34.0
python-multipart==0.0.20
fastapi-cache2==0.2.2
pickledb==1.2

py-cpuinfo==9.0.0
deepspeed==0.15.0 # for windows, download from https://pypi.org/project/deepspeed/0.15.0/#files

-r InternVL/requirements/internvl_chat.txt

# optional
# flash-attn==2.7.0.post2 # GET it from https://huggingface.co/lldacing/flash-attention-windows-wheel/tree/main

